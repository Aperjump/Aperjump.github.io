---
layout: post
title: "OpenCL notes_6 reduction"
description: "OpenCL notes_6 reduction"
categories: [OpenCL]
tags: [parallel, OpenCL]
redirect_from:
  - /2018/01/27/
---

## 1. Global size and Local size
The total number of work-items are called global size. The upper limit on the number of work-item you can generate is the maximum value of size_t. Another rule of thumb to follow when generating work-items is that the global size should be a multiple of the maximum work-group size. 
Because global memory access is time-consuimg, many kernels only access global memory twice: once to read input data into local memory, and once return computed value back. 
The number of work-items in a work-group is called the local size. 
An application can generate a nearly unlimited number of work-grouops, but the number that can execute in parallel is determined by the number of compute units on the device. 
The maximum work-group size depends on two things: the resources provided by the devce and the resources required by the kernel. 
```
cl_int clGetKernelWorkGroupInfo(cl_kernel kernel, cl_device_id device, cl_kernel_work_group_info para_name, size_t param_value_size, void* param_value, size_t *param_value_size_ret);
```
```
size_t wg_size;
err = clGetKernelWorkGroupInfo(kernel, device, CL_KERNEL_WORK_GROUP_SIZE, sizeof(wg_size), &wg_size, NULL);
```

## 2. Numerical Reduction
```
sum = 0.0f;
if (get_global_id(0) == 0) {
	for (int i = 0; i < 1048576; i++) {
		sum += data[i];
	}
}
```
```
__kernel void reduction_scalar(__global float* data, 
      __local float* partial_sums, __global float* output) {

   int lid = get_local_id(0);
   int group_size = get_local_size(0);

   partial_sums[lid] = data[get_global_id(0)];
   barrier(CLK_LOCAL_MEM_FENCE);

   for(int i = group_size/2; i>0; i >>= 1) {
      if(lid < i) {
         partial_sums[lid] += partial_sums[lid + i];
      }
      barrier(CLK_LOCAL_MEM_FENCE);
   }

   if(lid == 0) {
      output[get_group_id(0)] = partial_sums[0];
   }
}

__kernel void reduction_vector(__global float4* data, 
      __local float4* partial_sums, __global float* output) {

   int lid = get_local_id(0);
   int group_size = get_local_size(0);

   partial_sums[lid] = data[get_global_id(0)];
   barrier(CLK_LOCAL_MEM_FENCE);

   for(int i = group_size/2; i>0; i >>= 1) {
      if(lid < i) {
         partial_sums[lid] += partial_sums[lid + i];
      }
      barrier(CLK_LOCAL_MEM_FENCE);
   }

   if(lid == 0) {
      output[get_group_id(0)] = dot(partial_sums[0], (float4)(1.0f));
   }
}
```

## 3. Synchronizing work-groups
There is no way to sync work-groups. The only thing you can do is wait the kernel completes. For this reason, many openCL applications execute **multiple kernels** each successive kernel process the result generated by the kernel precediing it. 
```
__kernel void reduction_vector(__global float4* data, 
      __local float4* partial_sums) {

   int lid = get_local_id(0);
   int group_size = get_local_size(0);

   partial_sums[lid] = data[get_global_id(0)];
   barrier(CLK_LOCAL_MEM_FENCE);

   for(int i = group_size/2; i>0; i >>= 1) {
      if(lid < i) {
         partial_sums[lid] += partial_sums[lid + i];
      }
      barrier(CLK_LOCAL_MEM_FENCE);
   }

   if(lid == 0) {
      data[get_group_id(0)] = partial_sums[0];
   }
}

__kernel void reduction_complete(__global float4* data, 
      __local float4* partial_sums, __global float* sum) {

   int lid = get_local_id(0);
   int group_size = get_local_size(0);

   partial_sums[lid] = data[get_local_id(0)];
   barrier(CLK_LOCAL_MEM_FENCE);

   for(int i = group_size/2; i>0; i >>= 1) {
      if(lid < i) {
         partial_sums[lid] += partial_sums[lid + i];
      }
      barrier(CLK_LOCAL_MEM_FENCE);
   }

   if(lid == 0) {
      *sum = partial_sums[0].s0 + partial_sums[0].s1 +
             partial_sums[0].s2 + partial_sums[0].s3;
   }
}

```
