---
layout: post
title: "OpenCL notes_6 Mapreduce"
description: "OpenCL notes_6 Mapreduce"
categories: [OpenCL]
tags: [parallel, OpenCL]
redirect_from:
  - /2018/01/27/
---

## 1. MapReduce
A MapReduce solution contains two main stages: mapping and reduction. The processors involved in the mapping stage each receive input data and produce key-value pairs. Once the mapping is finished, each processor in the reduction stage receive values corresponding to a given key. The processors performing reduction process these values and merge them together to form the output data. 

![mapreduce_1](https://raw.githubusercontent.com/Aperjump/Aperjump.github.io/master/_picture/2018-01-27-OpenCL_7/OpenCL_7_1.PNG)

One implementation in openCL:
![mapreduce_2](https://raw.githubusercontent.com/Aperjump/Aperjump.github.io/master/_picture/2018-01-27-OpenCL_7/OpenCL_7_2.PNG)
Each block of local memory is specific to the work-items in a work-group. There will be multiple work-group colleborate. But, there is a problem: in a real mapreduce algo, there's no way in advance how many key-value pairs will be produced by the mapping stage. --> require **dynamic memory allocation** which cannot be accessed by openCL. 
Once solution is to comine the mapping and local reduction steps so that each work-group transfers **one** result to the global reduction stage.
Five steps:
1. Each work-item in a work-group performs the mapping, but instead of producing key-value pairs, it also a portion of the local reduction stage. 
2. The kernel executes a local barrier that prevents further execution untill all work-items in a work-group have completed their processing
3. In each work-group, the work-item whose local ID equals 0 reduces the work-group's output into a single result.
4. The kernel executes a global barrier that prevent further execution until all work-groups have completed their processing.
5. The work-item whose global ID equals 0 receives the result of each work-group and reduces this data to a final result. 
